---
title: "DM"
author: "Vivien & Louis"
date: "01/03/2021"
output:  html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Question 1


Le meilleur estimateur de ${f}(x)$ au sens de l'erreur quadratique moyenne intégrée est l'estimateur de Parzen-Rosenblatt $\hat{f}(x) = \frac{1}{nh_{cv}}\sum_{j=1}^{n}K(\frac{x-X_{j}}{h_{cv}})$ ou K est un noyau choisi et $h_{cv}$ petit obtenue par validation croisé *leave-one-out*.


```{r, echo=FALSE, message=FALSE}
library(readr)
devA <- read_csv("C:\\Users\\vivi1\\Documents\\Statistique_non_parametrique\\donnees_source\\devA.txt", col_names = FALSE)
```


```{r, echo = FALSE, warning=FALSE}
fn_chapeau <- density(devA$X1, bw="ucv", adjust=1, kernel="gaussian") 
plot(density(devA$X1, bw="ucv", adjust=1, kernel="gaussian"), main = "Estimation de f(x) par noyau gaussien")
plot(density(devA$X1, bw="sj", adjust=1, kernel="gaussian"), main = "Estimation de f(x) par noyau gaussien et fenêtre plug-in")
plot(density(devA$X1, bw="ucv", adjust=1, kernel="rectangular"), main = "Estimation de f(x) par noyau rectangulaire")
plot(density(devA$X1, bw="ucv", adjust=1, kernel="triangular"), main = "Estimation de f(x) par noyau triangulaire")
plot(density(devA$X1, bw="ucv", adjust=1, kernel="epanechnikov"), main = "Estimation de f(x) par noyau epanechnikov")
plot(density(devA$X1, bw="ucv", adjust=1, kernel="cosine"), main = "Estimation de f(x) par noyau cosine")
plot(density(devA$X1, bw="ucv", adjust=1, kernel="biweight"), main = "Estimation de f(x) par noyau biweight")
```



# Question 2


- Puisque l'on estime une loi de densité (semblant continue de par les nombreux chiffres après la virgule des observations) on souhaite que notre estimation ait les bonnes propriétés associées aux lois de densité. Ainsi il vient naturellement que le noyau $K$ doit être une densité de probabilité, lisse, continue et différentiable. Par défaut et sans information supplémentaire j'ai décidé de retenir le noyau gaussien. De plus, en faisant exception des valeurs extrêmes de notre echantillon, la partie centrale de la distribution de notre echantillon semble suivre une loi normale.

```{r, echo=FALSE}
qqnorm(devA$X1, main = "Echantillon devA", xlab = "Quantiles théoriques de loi normale", ylab = "Quantiles observés")
qqline(devA$X1)
```


- Le choix de la fenêtre $h$ est réalisé par validation croisée puisque $h_{cv} = arg min_{h}[\int \hat{f}^2_{n}(x)dx - \frac{2}{n}\sum_{i=1}^{n}\hat{f}_{-i}(X_{i})]$. $h_{}cv$ est alors égal à `r fn_chapeau$bw`. 



# Question 3 

$f(x)$ est une loi symétrique par rapport à $\theta_{0}$. Par conséquent, on peut estimer graphiquement $\theta_{0}$ par l'abscisse du point de maximum de la courbe. *i.e.*

```{r, echo=FALSE, warning=FALSE}
plot(density(devA$X1, bw="ucv", adjust=1, kernel="gaussian"), main = "Estimation de f(x) par noyau gaussien");
abline(v = fn_chapeau$x[which(fn_chapeau$y == max(fn_chapeau$y))], col = "red")

plot(density(devA$X1, bw="ucv", adjust=1, kernel="gaussian"), main = "Estimation de f(x) par noyau gaussien");
abline(v = mean(devA$X1), col = "red")
```

On approxime alors $\theta_{0}$ par $\theta_{0}^{approx} =$ `r fn_chapeau$x[which(fn_chapeau$y == max(fn_chapeau$y))]`
ou par la moyenne empirique : $\bar{X}_{n} =$ `r mean(devA$X1)` puisque que $\theta_{0}$ est l'espérance de la loi symétrique $f(x)$.


# Question 4 

On peut faire mieux que ces approximations :

$\bar{X}_n$


#Partie B
```{r,echo=FALSE}
dataB<-read.table("C:\\Users\\admin\\Documents\\Statistique_non_parametrique\\donnees_source\\devB.txt")

```


#Question 1

Sachant que les données observées sont issues d'une régression linéaire,que nos valeurs manquantes sont en fait des réponses et que l'hypothèse des observations MAR est satisfaite on a :

$Y=m(X)+eps, E(eps|X)=0$ et $Y||D,X$ si $D=0,1$

On souhaite estimer l'esperance de Y avec l'estimateur de Nadaraya-Watson:
$\hat r(u)=\frac{\sum Z_iK(\frac{u-U_i}{h})}{\sum K(\frac{u-U_i}{h})}$ avec K le noyau et h la fenêtre.

On sait que $P(X)=P(D=1|X)=E(D|X)$ donc on a :

$m(X)=E(Y|X)=\frac {E(DY|X)}{E(D|X)}$. On applique l'estimateur de Nadaraya-Watson sur E(DY|X) et E(D|X) et on obtient 

$E(DY|X)=\frac{\sum D_i Y_i K(.)}{\sum K(.)}$ et $E(D|X)=\hat p(X)=\frac{\sum D_i K(.)}{\sum K(.)}$

ce qui nous donne l'estimateur $\hat m_n$ de m en simplifiant:

$\hat p_n(x)=\frac{\sum D_i Y_i K(\frac {x-X_i}{h})}{\sum D_iK(\frac {x-X_i}{h})}$

On cherche à écrire un programme permettant de calculer l'estimateur $\hat \alpha_n$ de $\alpha_0 =E(Y)$. On a calculé deuc estimateurs de $\alpha_0$ tels que $\alpha1_n= \sum \hat m(X_i)/n$ et $\alpha2_n= \frac {\sum [\frac{D_i Y_i}{\hat p(X_i)}+(1-\frac {D_i}{\hat p(X_i)})\hat m(X_i)]}{n}$


```{r,echo=FALSE, warning=FALSE}
NW<-function(x,h=5){
  a<-c()
  kernel<-c()
  
  for (i in 1:200){
    if (is.na(dataB$V1[i])){
      kernel[i]<-0
      a[i]<-0}
    
    else{
      kernel[i]<-exp((((x-dataB$V2[i])/h)**2)/2)/sqrt(2*3.14)
      a[i]<-dataB$V1[i]*kernel[i]}
    
    
  }
  
  numerateur= sum(a)
  denominateur= sum(kernel)
  NaWa=numerateur/denominateur
  
  return(NaWa)
}


p_hat<-function(x,h=5){
  kernel<-c()
  kernel2<-c()
  
  for (i in 1:200){
    if (is.na(dataB$V1[i])){
      kernel[i]<-0
      }
    
    else{
      kernel[i]<-exp((((x-dataB$V2[i])/h)**2)/2)/sqrt(2*3.14)
      
      }
    
    kernel2[i]<-exp((((x-dataB$V2[i])/h)**2)/2)/sqrt(2*3.14)
    
  }
  
  numerateur= sum(kernel)
  denominateur= sum(kernel2)
  NaWa=numerateur/denominateur
  
  return(NaWa)
}NW<-function(x,h=5){
  a<-c()
  kernel<-c()
  
  for (i in 1:200){
    if (is.na(dataB$V1[i])){
      kernel[i]<-0
      a[i]<-0}
    
    else{
      kernel[i]<-exp((((x-dataB$V2[i])/h)**2)/2)/sqrt(2*3.14)
      a[i]<-dataB$V1[i]*kernel[i]}
    
    
  }
  
  numerateur= sum(a)
  denominateur= sum(kernel)
  NaWa=numerateur/denominateur
  
  return(NaWa)
}


p_hat<-function(x,h=5){
  kernel<-c()
  kernel2<-c()
  
  for (i in 1:200){
    if (is.na(dataB$V1[i])){
      kernel[i]<-0
      }
    
    else{
      kernel[i]<-exp((((x-dataB$V2[i])/h)**2)/2)/sqrt(2*3.14)
      
      }
    
    kernel2[i]<-exp((((x-dataB$V2[i])/h)**2)/2)/sqrt(2*3.14)
    
  }
  
  numerateur= sum(kernel)
  denominateur= sum(kernel2)
  NaWa=numerateur/denominateur
  
  return(NaWa)
}
```

Pour ceci, nous avons construit les fonctions retournant respectivement $\hat p$ et $\hat m$ pour un certain x et un paramètre de lissage h en appliquant la contrainte D=1.Nous avons choisi tout d'abord h=5 d'après un critère graphique mais nous calculerons sa valeur optimale par une validation croisée ultérieurement.
```{r,echo=FALSE}
phat<-sapply(dataB$V2,p_hat)
Nadi<-sapply(dataB$V2, NW)

alpha1_n<-sum(Nadi)/200


alpha2_n<-c()
for (i in 1:200){
  if (is.na(dataB$V1[i])){
    alpha2_n[i]<-Nadi[i]
  }
  else{
    alpha2_n[i]<-dataB$V1[i]/phat[i]+(1-1/phat[i])*Nadi[i]
  }
  
}
alpha2_n<-sum(alpha2_n)/200

```


Puis, il a fallu appliquer ces fonstions à l'ensemble des $X_i$ avec la fonction sapply() retournant ainsi un vecteur de valeurs.
Il suffit ensuite de construire les estimateurs $\alpha1_n$ et $\alpha2_n$ avec ces valeurs

Avec h=5 on obtient $\alpha1_n$ =`r alpha1_n` et $\alpha2_n$= `r alpha2_n` 

Nous calculons enuite le paramètre de lissage h optimal par validation croisée. l'estimation de h par la validation croisée s'écrit $h_CV=\argmin \frac{1}{n}






















```{r, echo=FALSE}
library(zoo)
dataB<-read.table("C:\\Users\\vivi1\\Documents\\Statistique_non_parametrique\\donnees_source\\devB.txt")

```


```{r }


dataB$V1<-na.aggregate(dataB$V1,fUN=median)


plot(ksmooth(dataB$V2,dataB$V1, kernel = "normal", bandwidth ="0.2",
             range.x = range(dataB$V2)),ylim=c(min(dataB$V1),max(dataB$V1)),col="red", type = "l")
lines(ksmooth(dataB$V2,dataB$V1, kernel = "normal", bandwidth = 1,
              range.x = range(dataB$V2)),col="blue")
lines(ksmooth(dataB$V2,dataB$V1, kernel = "normal", bandwidth = 0.5,
              range.x = range(dataB$V2)),col="green")
lines(ksmooth(dataB$V2,dataB$V1, kernel = "normal", bandwidth = 2,
              range.x = range(dataB$V2)),col="skyblue")
lines(ksmooth(dataB$V2,dataB$V1, kernel = "normal", bandwidth = 5,
              range.x = range(dataB$V2)),col="pink")
abline(h=0.5,col="black")

```


