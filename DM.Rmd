---
title: "DM"
author: "Vivien & Louis"
date: "01/03/2021"
output:  pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Question 1


Le meilleur estimateur de ${f}(x)$ au sens de l'erreur quadratique moyenne intégrée est l'estimateur de Parzen-Rosenblatt $\hat{f}(x) = \frac{1}{nh_{cv}}\sum_{j=1}^{n}K(\frac{x-X_{j}}{h_{cv}})$ ou K est un noyau choisi et $h_{cv}$ petit obtenue par validation croisé *leave-one-out*.


```{r, echo=FALSE, message=FALSE}
library(readr)
devA <- read_csv("C:\\Users\\admin\\Documents\\Statistique_non_parametrique\\donnees_source\\devA.txt", col_names = FALSE)
```



```{r, echo = FALSE, warning=FALSE}
fn_chapeau <- density(devA$X1, bw="ucv", adjust=1, kernel="gaussian") 
plot(density(devA$X1, bw="ucv", adjust=1, kernel="gaussian"), main = "Estimation de f(x) par noyau gaussien")
plot(density(devA$X1, bw="sj", adjust=1, kernel="gaussian"), main = "Estimation de f(x) par noyau gaussien et fenêtre plug-in")
plot(density(devA$X1, bw="ucv", adjust=1, kernel="rectangular"), main = "Estimation de f(x) par noyau rectangulaire")
plot(density(devA$X1, bw="ucv", adjust=1, kernel="triangular"), main = "Estimation de f(x) par noyau triangulaire")
plot(density(devA$X1, bw="ucv", adjust=1, kernel="epanechnikov"), main = "Estimation de f(x) par noyau epanechnikov")
plot(density(devA$X1, bw="ucv", adjust=1, kernel="cosine"), main = "Estimation de f(x) par noyau cosine")
plot(density(devA$X1, bw="ucv", adjust=1, kernel="biweight"), main = "Estimation de f(x) par noyau biweight")
```



# Question 2


- Puisque l'on estime une loi de densité (semblant continue de par les nombreux chiffres après la virgule des observations) on souhaite que notre estimation ait les bonnes propriétés associées aux lois de densité. Ainsi il vient naturellement que le noyau $K$ doit être une densité de probabilité, lisse, continue et différentiable. Par défaut et sans information supplémentaire j'ai décidé de retenir le noyau gaussien. De plus, en faisant exception des valeurs extrêmes de notre echantillon, la partie centrale de la distribution de notre echantillon semble suivre une loi normale.

```{r, echo=FALSE}
qqnorm(devA$X1, main = "Echantillon devA", xlab = "Quantiles théoriques de loi normale", ylab = "Quantiles observés")
qqline(devA$X1)
```


- Le choix de la fenêtre $h$ est réalisé par validation croisée puisque $h_{cv} = arg min_{h}[\int \hat{f}^2_{n}(x)dx - \frac{2}{n}\sum_{i=1}^{n}\hat{f}_{-i}(X_{i})]$. $h_{}cv$ est alors égal à `r fn_chapeau$bw`. 



# Question 3 

$f(x)$ est une loi symétrique par rapport à $\theta_{0}$. Par conséquent, on peut estimer graphiquement $\theta_{0}$ par l'abscisse du point de maximum de la courbe. *i.e.*

```{r, echo=FALSE, warning=FALSE}
plot(density(devA$X1, bw="ucv", adjust=1, kernel="gaussian"), main = "Estimation de f(x) par noyau gaussien");
abline(v = fn_chapeau$x[which(fn_chapeau$y == max(fn_chapeau$y))], col = "red")

plot(density(devA$X1, bw="ucv", adjust=1, kernel="gaussian"), main = "Estimation de f(x) par noyau gaussien");
abline(v = mean(devA$X1), col = "red")
```

On approxime alors $\theta_{0}$ par $\theta_{0}^{approx} =$ `r fn_chapeau$x[which(fn_chapeau$y == max(fn_chapeau$y))]`
ou par la moyenne empirique : $\bar{X}_{n} =$ `r mean(devA$X1)` puisque que $\theta_{0}$ est l'espérance de la loi symétrique $f(x)$.


# Question 4 

On peut faire mieux que ces approximations :

```{r, echo=FALSE}

```



#Partie B
```{r,echo=FALSE}
dataB<-read.table("C:\\Users\\admin\\Documents\\Statistique_non_parametrique\\donnees_source\\devB.txt")

```


#Question 1

Sachant que les données observées sont issues d'une régression linéaire,que nos valeurs manquantes sont en fait des réponses et que l'hypothèse des observations MAR est satisfaite on a :

$Y=m(X)+eps, E(eps|X)=0$ et $Y||D,X$ si $D=0,1$

On souhaite estimer l'esperance de Y avec l'estimateur de Nadaraya-Watson:
$\hat r(u)=\frac{\sum Z_iK(\frac{u-U_i}{h})}{\sum K(\frac{u-U_i}{h})}$ avec K le noyau et h la fenêtre.

On sait que $P(X)=P(D=1|X)=E(D|X)$ donc on a :

$m(X)=E(Y|X)=\frac {E(DY|X)}{E(D|X)}$. On applique l'estimateur de Nadaraya-Watson sur E(DY|X) et E(D|X) et on obtient 

$E(DY|X)=\frac{\sum D_i Y_i K(.)}{\sum K(.)}$ et $E(D|X)=\hat p(X)=\frac{\sum D_i K(.)}{\sum K(.)}$

ce qui nous donne l'estimateur $\hat m_n$ de m en simplifiant:

$\hat p_n(x)=\frac{\sum D_i Y_i K(\frac {x-X_i}{h})}{\sum D_iK(\frac {x-X_i}{h})}$

On cherche à écrire un programme permettant de calculer l'estimateur $\hat \alpha_n$ de $\alpha_0 =E(Y)$. On a calculé deuc estimateurs de $\alpha_0$ tels que $\alpha1_n= \sum \hat m(X_i)/n$ et $\alpha2_n= \frac {\sum [\frac{D_i Y_i}{\hat p(X_i)}+(1-\frac {D_i}{\hat p(X_i)})\hat m(X_i)]}{n}$


```{r,echo=FALSE, warning=FALSE}
NW<-function(x,h=5){
  a<-c()
  kernel<-c()
  
  for (i in 1:200){
    if (is.na(dataB$V1[i])){
      kernel[i]<-0
      a[i]<-0}
    
    else{
      kernel[i]<-exp(-(((x-dataB$V2[i])/h)**2)/2)/sqrt(2*3.14)
      a[i]<-dataB$V1[i]*kernel[i]}
    
    
  }
  
  numerateur= sum(a)
  denominateur= sum(kernel)
  NaWa=numerateur/denominateur
  
  return(NaWa)
}


p_hat<-function(x,h=5){
  kernel<-c()
  kernel2<-c()
  
  for (i in 1:200){
    if (is.na(dataB$V1[i])){
      kernel[i]<-0
      }
    
    else{
      kernel[i]<-exp(-(((x-dataB$V2[i])/h)**2)/2)/sqrt(2*3.14)
      
      }
    
    kernel2[i]<-exp(-(((x-dataB$V2[i])/h)**2)/2)/sqrt(2*3.14)
    
  }
  
  numerateur= sum(kernel)
  denominateur= sum(kernel2)
  NaWa=numerateur/denominateur
  
  return(NaWa)
}NW<-function(x,h=5){
  a<-c()
  kernel<-c()
  
  for (i in 1:200){
    if (is.na(dataB$V1[i])){
      kernel[i]<-0
      a[i]<-0}
    
    else{
      kernel[i]<-exp(-(((x-dataB$V2[i])/h)**2)/2)/sqrt(2*3.14)
      a[i]<-dataB$V1[i]*kernel[i]}
    
    
  }
  
  numerateur= sum(a)
  denominateur= sum(kernel)
  NaWa=numerateur/denominateur
  
  return(NaWa)
}


p_hat<-function(x,h=5){
  kernel<-c()
  kernel2<-c()
  
  for (i in 1:200){
    if (is.na(dataB$V1[i])){
      kernel[i]<-0
      }
    
    else{
      kernel[i]<-exp(-(((x-dataB$V2[i])/h)**2)/2)/sqrt(2*3.14)
      
      }
    
    kernel2[i]<-exp(-(((x-dataB$V2[i])/h)**2)/2)/sqrt(2*3.14)
    
  }
  
  numerateur= sum(kernel)
  denominateur= sum(kernel2)
  NaWa=numerateur/denominateur
  
  return(NaWa)
}
```

Pour ceci, nous avons construit les fonctions retournant respectivement $\hat p$ et $\hat m$ pour un certain x et un paramètre de lissage h en appliquant la contrainte D=1.Nous avons choisi tout d'abord h=5 d'après un critère graphique mais nous calculerons sa valeur optimale par une validation croisée ultérieurement.
```{r,echo=FALSE}
phat<-sapply(dataB$V2,p_hat)
Nadi<-sapply(dataB$V2, NW)

alpha1_n<-sum(Nadi)/200


alpha2_n<-c()
for (i in 1:200){
  if (is.na(dataB$V1[i])){
    alpha2_n[i]<-Nadi[i]
  }
  else{
    alpha2_n[i]<-dataB$V1[i]/phat[i]+(1-1/phat[i])*Nadi[i]
  }
  
}
alpha2_n<-sum(alpha2_n)/200

```


Puis, il a fallu appliquer ces fonctions à l'ensemble des $X_i$ avec la fonction sapply() retournant ainsi un vecteur de valeurs.
Il suffit ensuite de construire les estimateurs $\alpha1_n$ et $\alpha2_n$ avec ces valeurs

Avec h=5 on obtient $\alpha1_n$ =`r alpha1_n` et $\alpha2_n$= `r alpha2_n` 

Nous calculons enuite le paramètre de lissage h optimal par validation croisée.


#mettre l'explication de h_p_cv
Calculons tout d'abord l'estimateur $h_{p_{CV}}$ le paramètre de lissage de $\hat p$.On a:
$h_{p_{CV}}=argmin \frac{1}{n}\sum[D_i- \hat p_{n,h}^{(-i)}(X_i)]^2 $

avec  $\hat p_{n,h}^{(-i)}=\frac{\sum\limits_{j\neq i}^n D_j K(.)}{\sum \limits_{j\neq i}^n K(.)} $ 
#expliquer la loi des gds nombres
```{r}
P_moinsI<-function(x,h=1,k){
  D<-c()
  p<-c()
  kernel<-c()
  for (i in 1:200){
    kernel[i]<-exp(-(((x-dataB$V2[i])/h)**2)/2)/sqrt(2*pi)
    if (is.na(dataB$V1[i])){
      D[i]<-0
      p[i]<-0}
    else{
      D[i]<-1
      p[i]<-D[i]*kernel[i]}
  }
  p<-p[-k]
  
  numerateur= sum(p)
  denominateur= sum(kernel)
  NaWa=numerateur/denominateur
  
  return(NaWa)
  
}



h_p_cv<-function(h){
  summum<-c()
  for (k in 1:200){
    if (is.na(dataB$V1[k])){
      summum[k]<-P_moinsI(dataB$V2[k],h,k)**2
    }
    else{  
      summum[k]<-(1-P_moinsI(dataB$V2[k],h,k))**2
    }}
  return(sum(summum)/length(summum))
}
```
On construit un premier algorythme pour déterminer les  $\hat p_{n,h}^{(-i)}$ avec une fonction prenant en argument une observation x, un paramètre de lissage h=1 qui variera à l'avenir et un paramètre k permettant de retirer l'observation i. Cette fonction retourne la valeur de $\hat p_{n,h}^{(-i)}$ pour un $x_i$ fixé en vérifiant si $Y_i$ est observé.

Une deuxième fonction prenant en argument h pour faire varier le paramètre de lissage et retournant la valeur $h_{p_{CV}}$ applique la fonction ci-dessus en différençiant les 200 $Y_i$ observés ou non.   

```{r}
sequence<-seq(1,20,1)
h_p_cv(Inf)
estim_hp_cv<-sapply(sequence,h_p_cv)


min_hp_cv<-estim_hp_cv[1]
h_p_min<-sequence[1]
for (i in 1:length(estim_hp_cv)){
  
  if (estim_hp_cv[i]<min_hp_cv){
    min_hp_cv<-estim_hp_cv[i]
    h_p_min<-sequence[i]
  } 
}
print(h_p_min)
sequence<-seq(0.1,2,0.1)
sequence<-seq(1,1.2,0.001)
```
En appliquant à la dernière fonction plusieurs séquences de h différents, on essaye de determiner une approximation de h optimale c-a-d la valeur de h pour laquelle $h_{p_{CV}}$ est minimal. On trouve donc pour $h_{p_{CV}}$ la valeur
`r h_p_min`

l'estimation de h pour l'estimateur $\hat m$ par la validation croisée s'écrit$h_{CV}=argmin \frac{1}{n}\sum[\frac{Y_i}{\hat p_{n,h}^{(-i)}}- \hat m_{n,h}^{(-i)}(X_i)]^2 $

 
$\hat m_{n,h}^{(-i)}=\frac{\sum\limits_{j\neq i}^n Y_j K(.)}{\sum \limits_{j\neq i}^n K(.)} $


Or dans notre cas, nous avons des valeurs manquantes. Mais grâce à la loi des grands nombres on peut écrire:
$h_{CV}=argmin \frac{1}{n}\sum[D_iY_i- \hat m_{n,h}^{(-i)}(X_i)]^2 $ avec $\hat m_{n,h}^{(-i)}= \frac{\sum \limits_{j \neq i}^n  D_jK(.)}{\sum\limits_{j\neq i}^n D_jK(.)} $
#les argmin marchent pas 

Pour construire un algorithme permettant de determiner une approximation de h, il est néccessaire de construire $\hat m_{n,h}^{(-i)}$

```{r,echo=FALSE}
NW_moinsI<-function(x,h=1,k){
  a<-c()
  kernel<-c()
  
  for (i in 1:200){
    if (is.na(dataB$V1[i])){
      kernel[i]<-0
      a[i]<-0}
    
    else{
      kernel[i]<-exp(-(((x-dataB$V2[i])/h)**2)/2)/sqrt(2*pi)
      a[i]<-dataB$V1[i]*kernel[i]}
    }
    
  a<-a[-k]
  kernel<-kernel[-k]
  numerateur= sum(a)
  denominateur= sum(kernel)
  NaWa=numerateur/denominateur
  
  return(NaWa)
}

h_cv<-function(h){
  summum<-c()
  for (k in 1:200){
    if (is.na(dataB$V1[k])){
      summum[k]<-NW_moinsI(dataB$V2[k],h,k)**2
    }
    else{  
      summum[k]<-(dataB$V1[k]/P_moinsI(dataB$V2[k],h=1.069,k=k)-NW_moinsI(dataB$V2[k],h,k))**2
    }}
  summum=summum[summum!=0]
  return(sum(summum)/length(summum))
}

```

Dans la même logique que pour $h_{p_{CV}}$, on construit un premier algorythme pour déterminer les  $\hat m_{n,h}^{(-i)}$ avec une fonction prenant en argument une observation x, un paramètre de lissage h=1 qui variera à l'avenir et un paramètre k permettant de retirer l'observation i. Cette fonction retourne la valeur de $\hat m_{n,h}^{(-i)}$ pour un $x_i$ fixé en vérifiant si $Y_i$ est observé.

Une deuxième fonction prenant en argument h pour faire varier le paramètre de lissage et retournant la valeur $h_{CV}$ applique la fonction ci-dessus en différençiant les 200 $Y_i$ observés ou non. Dans cette fonction, on applique la valeur optimale de $h_{p_{CV}}$ pour la valeur de $\hat p_{n,h}^{(-i)}$ calculée prédédemment. 

```{r}
sequence<-seq(0.1,1,0.1)
estim_hcv<-sapply(sequence,h_cv)


min_hcv<-estim_hcv[1]
h_min<-sequence[1]
for (i in 1:length(estim_hcv)){
  
  if (estim_hcv[i]<min_hcv){
    min_hcv<-estim_hcv[i]
    h_min<-sequence[i]
  } 
}
print(h_min)

sequence<-seq(2,4,0.01)
sequence<-seq(3.10,3.12,0.0001)
```


En appliquant à la dernière fonction plusieurs séquences de h différents, on essaye de determiner une approximation de h optimale c-a-d la valeur de h pour laquelle $h_{CV}$ est minimal. On trouve donc pour $h_{CV}$ la valeur
`r h_min`

On remplace ainsi ensuite la valeur h=5 par h=`r h_p_min` pour $\hat p$ et h=`r h_min` pour $\hat m$ dans les fonctions permettant de calculer ces estimateurs

```{r}
NW<-function(x,h=h_validcroisee){
  a<-c()
  kernel<-c()
  
  for (i in 1:200){
    if (is.na(dataB$V1[i])){
      kernel[i]<-0
      a[i]<-0}
    
    else{
      kernel[i]<-exp(-(((x-dataB$V2[i])/h)**2)/2)/sqrt(2*pi)
      a[i]<-dataB$V1[i]*kernel[i]}
    
    
  }
  
  numerateur= sum(a)
  denominateur= sum(kernel)
  NaWa=numerateur/denominateur
  
  return(NaWa)
}


p_hat<-function(x,h=h_validcroisee){
  kernel<-c()
  kernel2<-c()
  
  for (i in 1:200){
    if (is.na(dataB$V1[i])){
      kernel[i]<-0
    }
    
    else{
      kernel[i]<-exp(-(((x-dataB$V2[i])/h)**2)/2)/sqrt(2*pi)
      
    }
    
    kernel2[i]<-exp(-(((x-dataB$V2[i])/h)**2)/2)/sqrt(2*pi)
    
  }
  
  numerateur= sum(kernel)
  denominateur= sum(kernel2)
  NaWa=numerateur/denominateur
  
  return(NaWa)
}
phat<-sapply(dataB$V2,p_hat)
Nadi<-sapply(dataB$V2,NW)

theta_tild1<-sum(Nadi)/200


theta_tild2<-c()
for (i in 1:200){
  if (is.na(dataB$V1[i])){
    theta_tild2[i]<-Nadi[i]
  }
  else{
    theta_tild2[i]<-dataB$V1[i]/phat[i]+(1-1/phat[i])*Nadi[i]
  }
  
}
theta_tild2<-sum(theta_tild2)/200

```


 on obtient $\alpha1_n$ =`r theta_tild1` et $\alpha2_n$= `r theta_tild2` 










```{r}
library(zoo)
dataB<-read.table("C:\\Users\\admin\\Documents\\Statistique_non_parametrique\\donnees_source\\devB.txt")


dataB$V1<-na.aggregate(dataB$V1,fUN=median)


plot(ksmooth(dataB$V2,dataB$V1, kernel = "normal", bandwidth =0.2,
             range.x = range(dataB$V2)),ylim=c(min(dataB$V1),max(dataB$V1)),col="red")
lines(ksmooth(dataB$V2,dataB$V1, kernel = "normal", bandwidth = 1,
              range.x = range(dataB$V2)),col="blue")
lines(ksmooth(dataB$V2,dataB$V1, kernel = "normal", bandwidth = 0.5,
              range.x = range(dataB$V2)),col="green")
lines(ksmooth(dataB$V2,dataB$V1, kernel = "normal", bandwidth = 2,
              range.x = range(dataB$V2)),col="skyblue")
lines(ksmooth(dataB$V2,dataB$V1, kernel = "normal", bandwidth = 5,
              range.x = range(dataB$V2)),col="pink")
abline(h=0.5,col="black")

```


